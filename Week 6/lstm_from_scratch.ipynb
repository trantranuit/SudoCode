{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17aTBUSVnwBL",
    "outputId": "0e718913-836a-498a-9fe6-cf2c08a9bbe6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import trange\n",
    "import gc\n",
    "\n",
    "# Natural Language Processing libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading & Vocabulary Building\n",
    "\n",
    "Xử lý dữ liệu văn bản lớn theo chunks và xây dựng vocabulary với memory-efficient approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U26yYWSsn1a_",
    "outputId": "f6fe2c41-a986-40e2-9607-4198ad797a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "  Processed 10 chunks...\n",
      "  Processed 20 chunks...\n",
      "  Processed 30 chunks...\n",
      "  Processed 40 chunks...\n",
      "  Processed 50 chunks...\n",
      "  Processed 60 chunks...\n",
      "  Processed 70 chunks...\n",
      "  Processed 80 chunks...\n",
      "  Processed 90 chunks...\n",
      "  Processed 100 chunks...\n",
      "  Processed 110 chunks...\n",
      "  Processed 120 chunks...\n",
      "  Processed 130 chunks...\n",
      "  Processed 140 chunks...\n",
      "  Processed 150 chunks...\n",
      "  Processed 160 chunks...\n",
      "  Processed 170 chunks...\n",
      "  Processed 180 chunks...\n",
      "  Processed 190 chunks...\n",
      "  Processed 200 chunks...\n",
      "  Processed 210 chunks...\n",
      "  Processed 220 chunks...\n",
      "  Processed 230 chunks...\n",
      "  Processed 240 chunks...\n",
      "  Processed 250 chunks...\n",
      "  Processed 260 chunks...\n",
      "  Processed 270 chunks...\n",
      "  Processed 280 chunks...\n",
      "  Processed 290 chunks...\n",
      "  Processed 300 chunks...\n",
      "  Processed 310 chunks...\n",
      "  Processed 320 chunks...\n",
      "  Processed 330 chunks...\n",
      "  Processed 340 chunks...\n",
      "  Processed 350 chunks...\n",
      "  Processed 360 chunks...\n",
      "  Processed 370 chunks...\n",
      "  Processed 380 chunks...\n",
      "  Processed 390 chunks...\n",
      "  Processed 400 chunks...\n",
      "  Processed 410 chunks...\n",
      "  Processed 420 chunks...\n",
      "  Processed 430 chunks...\n",
      "  Processed 440 chunks...\n",
      "  Processed 450 chunks...\n",
      "  Processed 460 chunks...\n",
      "  Processed 470 chunks...\n",
      "  Processed 480 chunks...\n",
      "  Processed 490 chunks...\n",
      "  Processed 500 chunks...\n",
      "  Processed 510 chunks...\n",
      "  Processed 520 chunks...\n",
      "  Processed 530 chunks...\n",
      "  Processed 540 chunks...\n",
      "  Processed 550 chunks...\n",
      "  Processed 560 chunks...\n",
      "  Processed 570 chunks...\n",
      "  Processed 580 chunks...\n",
      "  Processed 590 chunks...\n",
      "  Processed 600 chunks...\n",
      "  Processed 610 chunks...\n",
      "  Processed 620 chunks...\n",
      "  Processed 630 chunks...\n",
      "  Processed 640 chunks...\n",
      "  Processed 650 chunks...\n",
      "  Processed 660 chunks...\n",
      "  Processed 670 chunks...\n",
      "  Processed 680 chunks...\n",
      "  Processed 690 chunks...\n",
      "  Processed 700 chunks...\n",
      "  Processed 710 chunks...\n",
      "  Processed 720 chunks...\n",
      "  Processed 730 chunks...\n",
      "  Processed 740 chunks...\n",
      "  Processed 750 chunks...\n",
      "  Processed 760 chunks...\n",
      "  Processed 770 chunks...\n",
      "  Processed 780 chunks...\n",
      "  Processed 790 chunks...\n",
      "  Processed 800 chunks...\n",
      "  Processed 810 chunks...\n",
      "  Processed 820 chunks...\n",
      "  Processed 830 chunks...\n",
      "  Processed 840 chunks...\n",
      "  Processed 850 chunks...\n",
      "  Processed 860 chunks...\n",
      "  Processed 870 chunks...\n",
      "  Processed 880 chunks...\n",
      "  Processed 890 chunks...\n",
      "  Processed 900 chunks...\n",
      "  Processed 910 chunks...\n",
      "  Processed 920 chunks...\n",
      "  Processed 930 chunks...\n",
      "  Processed 940 chunks...\n",
      "  Processed 950 chunks...\n",
      "  Processed 960 chunks...\n",
      "  Processed 970 chunks...\n",
      "  Processed 980 chunks...\n",
      "  Processed 990 chunks...\n",
      "  Processed 1000 chunks...\n",
      "  Processed 1010 chunks...\n",
      "  Processed 1020 chunks...\n",
      "  Processed 1030 chunks...\n",
      "  Processed 1040 chunks...\n",
      "  Processed 1050 chunks...\n",
      "  Processed 1060 chunks...\n",
      "  Processed 1070 chunks...\n",
      "  Processed 1080 chunks...\n",
      "  Processed 1090 chunks...\n",
      "  Processed 1100 chunks...\n",
      "  Processed 1110 chunks...\n",
      "  Processed 1120 chunks...\n",
      "  Processed 1130 chunks...\n",
      "  Processed 1140 chunks...\n",
      "  Processed 1150 chunks...\n",
      "  Processed 1160 chunks...\n",
      "  Processed 1170 chunks...\n",
      "  Processed 1180 chunks...\n",
      "  Processed 1190 chunks...\n",
      "Found 1383309 unique words\n",
      "Vocabulary size: 800\n",
      "  Converting chunk 10...\n",
      "  Converting chunk 20...\n",
      "  Converting chunk 30...\n",
      "  Converting chunk 40...\n",
      "  Converting chunk 50...\n",
      "  Converting chunk 60...\n",
      "  Converting chunk 70...\n",
      "  Converting chunk 80...\n",
      "  Converting chunk 90...\n",
      "  Converting chunk 100...\n",
      "  Converting chunk 110...\n",
      "  Converting chunk 120...\n",
      "  Converting chunk 130...\n",
      "  Converting chunk 140...\n",
      "  Converting chunk 150...\n",
      "  Converting chunk 160...\n",
      "  Converting chunk 170...\n",
      "  Converting chunk 180...\n",
      "  Converting chunk 190...\n",
      "  Converting chunk 200...\n",
      "  Converting chunk 210...\n",
      "  Converting chunk 220...\n",
      "  Converting chunk 230...\n",
      "  Converting chunk 240...\n",
      "  Converting chunk 250...\n",
      "  Converting chunk 260...\n",
      "  Converting chunk 270...\n",
      "  Converting chunk 280...\n",
      "  Converting chunk 290...\n",
      "  Converting chunk 300...\n",
      "  Converting chunk 310...\n",
      "  Converting chunk 320...\n",
      "  Converting chunk 330...\n",
      "  Converting chunk 340...\n",
      "  Converting chunk 350...\n",
      "  Converting chunk 360...\n",
      "  Converting chunk 370...\n",
      "  Converting chunk 380...\n",
      "  Converting chunk 390...\n",
      "  Converting chunk 400...\n",
      "  Converting chunk 410...\n",
      "  Converting chunk 420...\n",
      "  Converting chunk 430...\n",
      "  Converting chunk 440...\n",
      "  Converting chunk 450...\n",
      "  Converting chunk 460...\n",
      "  Converting chunk 470...\n",
      "  Converting chunk 480...\n",
      "  Converting chunk 490...\n",
      "  Converting chunk 500...\n",
      "  Converting chunk 510...\n",
      "  Converting chunk 520...\n",
      "  Converting chunk 530...\n",
      "  Converting chunk 540...\n",
      "  Converting chunk 550...\n",
      "  Converting chunk 560...\n",
      "  Converting chunk 570...\n",
      "  Converting chunk 580...\n",
      "  Converting chunk 590...\n",
      "  Converting chunk 600...\n",
      "  Converting chunk 610...\n",
      "  Converting chunk 620...\n",
      "  Converting chunk 630...\n",
      "  Converting chunk 640...\n",
      "  Converting chunk 650...\n",
      "  Converting chunk 660...\n",
      "  Converting chunk 670...\n",
      "  Converting chunk 680...\n",
      "  Converting chunk 690...\n",
      "  Converting chunk 700...\n",
      "  Converting chunk 710...\n",
      "  Converting chunk 720...\n",
      "  Converting chunk 730...\n",
      "  Converting chunk 740...\n",
      "  Converting chunk 750...\n",
      "  Converting chunk 760...\n",
      "  Converting chunk 770...\n",
      "  Converting chunk 780...\n",
      "  Converting chunk 790...\n",
      "  Converting chunk 800...\n",
      "  Converting chunk 810...\n",
      "  Converting chunk 820...\n",
      "  Converting chunk 830...\n",
      "  Converting chunk 840...\n",
      "  Converting chunk 850...\n",
      "  Converting chunk 860...\n",
      "  Converting chunk 870...\n",
      "  Converting chunk 880...\n",
      "  Converting chunk 890...\n",
      "  Converting chunk 900...\n",
      "  Converting chunk 910...\n",
      "  Converting chunk 920...\n",
      "  Converting chunk 930...\n",
      "  Converting chunk 940...\n",
      "  Converting chunk 950...\n",
      "  Converting chunk 960...\n",
      "  Converting chunk 970...\n",
      "  Converting chunk 980...\n",
      "  Converting chunk 990...\n",
      "  Converting chunk 1000...\n",
      "  Converting chunk 1010...\n",
      "  Converting chunk 1020...\n",
      "  Converting chunk 1030...\n",
      "  Converting chunk 1040...\n",
      "  Converting chunk 1050...\n",
      "  Converting chunk 1060...\n",
      "  Converting chunk 1070...\n",
      "  Converting chunk 1080...\n",
      "  Converting chunk 1090...\n",
      "  Converting chunk 1100...\n",
      "  Converting chunk 1110...\n",
      "  Converting chunk 1120...\n",
      "  Converting chunk 1130...\n",
      "  Converting chunk 1140...\n",
      "  Converting chunk 1150...\n",
      "  Converting chunk 1160...\n",
      "  Converting chunk 1170...\n",
      "  Converting chunk 1180...\n",
      "  Converting chunk 1190...\n",
      "  Total tokens processed: 286,270,442\n",
      "Created 286,270,442 token indices\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"data_full.txt\"  \n",
    "\n",
    "def build_vocab_from_chunks(filename, max_vocab=800, chunk_size=1024*1024):\n",
    "    \"\"\"\n",
    "    Build vocabulary from large text files by processing in chunks.\n",
    "    \n",
    "    Args:\n",
    "        filename: Path to text file\n",
    "        max_vocab: Maximum vocabulary size\n",
    "        chunk_size: Size of each chunk in bytes\n",
    "    \n",
    "    Returns:\n",
    "        word_counts: Counter object with word frequencies\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        chunk_num = 0\n",
    "        while True:\n",
    "            # Read file in chunks to handle large files\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "\n",
    "            chunk_num += 1\n",
    "            if chunk_num % 10 == 0:\n",
    "                print(f\"  Processed {chunk_num} chunks...\")\n",
    "\n",
    "            # Tokenize chunk and update word counts\n",
    "            chunk_tokens = re.findall(r'\\b\\w+\\b', chunk.lower())\n",
    "            word_counts.update(chunk_tokens)\n",
    "\n",
    "            # Clean up memory\n",
    "            del chunk, chunk_tokens\n",
    "            gc.collect()\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "# Build vocabulary with frequency-based filtering\n",
    "print(\"Building vocabulary from chunks...\")\n",
    "word_counts = build_vocab_from_chunks(filename, max_vocab=800)\n",
    "print(f\"Found {len(word_counts)} unique words\")\n",
    "\n",
    "# Select most frequent words for vocabulary\n",
    "most_common = word_counts.most_common(799)  # Reserve 1 slot for <UNK>\n",
    "vocab = [w for w, _ in most_common]\n",
    "vocab.append('<UNK>')  # Unknown word token\n",
    "\n",
    "# Create word-to-index mappings\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Clean up memory\n",
    "del word_counts, most_common\n",
    "gc.collect()\n",
    "\n",
    "def text_to_indices_chunked(filename, word_to_idx, chunk_size=1024*1024):\n",
    "    \"\"\"\n",
    "    Convert text file to token indices using chunked processing.\n",
    "    \n",
    "    Args:\n",
    "        filename: Path to text file\n",
    "        word_to_idx: Dictionary mapping words to indices\n",
    "        chunk_size: Size of each chunk in bytes\n",
    "    \n",
    "    Returns:\n",
    "        all_indices: List of token indices\n",
    "    \"\"\"\n",
    "    all_indices = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        chunk_num = 0\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "\n",
    "            chunk_num += 1\n",
    "            if chunk_num % 10 == 0:\n",
    "                print(f\"  Converting chunk {chunk_num}...\")\n",
    "\n",
    "            # Tokenize and convert to indices\n",
    "            chunk_tokens = re.findall(r'\\b\\w+\\b', chunk.lower())\n",
    "            \n",
    "            for token in chunk_tokens:\n",
    "                idx = word_to_idx.get(token, word_to_idx['<UNK>'])\n",
    "                all_indices.append(idx)\n",
    "\n",
    "            total_tokens += len(chunk_tokens)\n",
    "\n",
    "            # Clean up memory\n",
    "            del chunk, chunk_tokens\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"  Total tokens processed: {total_tokens:,}\")\n",
    "    return all_indices\n",
    "\n",
    "# Convert entire text to token indices\n",
    "print(\"Converting text to indices...\")\n",
    "token_indices = text_to_indices_chunked(filename, word_to_idx)\n",
    "print(f\"Created {len(token_indices):,} token indices\")\n",
    "\n",
    "# Final memory cleanup\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "\n",
    "Tạo batches dữ liệu training hiệu quả cho mô hình LSTM với sequence-to-sequence format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipA9sGRJn1eb",
    "outputId": "f5cdff00-54bd-423d-fd9d-676ad6134e28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple data generator ready\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 20  # Length of input sequences\n",
    "\n",
    "def simple_data_generator(token_indices, batch_size=64):\n",
    "    \"\"\"\n",
    "    Generate training batches for sequence-to-sequence learning.\n",
    "    \n",
    "    Args:\n",
    "        token_indices: List of token indices from preprocessed text\n",
    "        batch_size: Number of sequences per batch\n",
    "        \n",
    "    Yields:\n",
    "        Xb: Input sequences of shape (batch_size, SEQ_LENGTH)\n",
    "        Yb: Target sequences of shape (batch_size, SEQ_LENGTH) \n",
    "            (shifted by 1 position for next-word prediction)\n",
    "    \"\"\"\n",
    "    n = len(token_indices)\n",
    "    # Convert to numpy array for efficient indexing\n",
    "    indices_array = np.array(token_indices, dtype=np.int32)\n",
    "\n",
    "    while True:\n",
    "        # Generate random starting positions for sequences\n",
    "        max_start = n - SEQ_LENGTH - 1\n",
    "        starts = np.random.randint(0, max_start, batch_size)\n",
    "\n",
    "        # Initialize batch arrays\n",
    "        Xb = np.zeros((batch_size, SEQ_LENGTH), dtype=np.int32)\n",
    "        Yb = np.zeros((batch_size, SEQ_LENGTH), dtype=np.int32)\n",
    "\n",
    "        # Fill batch with sequences and their shifted targets\n",
    "        for i, s in enumerate(starts):\n",
    "            Xb[i] = indices_array[s:s + SEQ_LENGTH]           # Input sequence\n",
    "            Yb[i] = indices_array[s + 1:s + SEQ_LENGTH + 1]   # Target (shifted by 1)\n",
    "\n",
    "        yield Xb, Yb\n",
    "\n",
    "print(\"Data generator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model Implementation\n",
    "\n",
    "Mô hình LSTM đơn giản nhưng hiệu quả với forward pass, backpropagation và gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeM_dFn9n1hA",
    "outputId": "82d1ce50-fe3c-451a-9825-9adc3322b5d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple LSTM ready\n"
     ]
    }
   ],
   "source": [
    "class SimpleLSTM:\n",
    "    \"\"\"\n",
    "    Efficient LSTM implementation optimized for text generation.\n",
    "    \n",
    "    Features:\n",
    "    - Memory-efficient training with gradient clipping\n",
    "    - Numerical stability with proper weight initialization\n",
    "    - Sequence-to-sequence learning capability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim=256, lr=0.02):\n",
    "        \"\"\"\n",
    "        Initialize LSTM model parameters.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            hidden_dim: Dimension of hidden states\n",
    "            lr: Learning rate\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = 128  # Word embedding dimension\n",
    "        self.lr = lr\n",
    "\n",
    "        # Xavier-style initialization scale\n",
    "        scale = 0.08\n",
    "\n",
    "        # Word embeddings matrix\n",
    "        self.E = np.random.uniform(-scale, scale, (vocab_size, self.emb_dim)).astype(np.float32)\n",
    "\n",
    "        # LSTM gate weight matrices\n",
    "        input_size = self.emb_dim + hidden_dim\n",
    "        self.Wf = np.random.uniform(-scale, scale, (input_size, hidden_dim)).astype(np.float32)  # Forget gate\n",
    "        self.Wi = np.random.uniform(-scale, scale, (input_size, hidden_dim)).astype(np.float32)  # Input gate\n",
    "        self.Wc = np.random.uniform(-scale, scale, (input_size, hidden_dim)).astype(np.float32)  # Candidate values\n",
    "        self.Wo = np.random.uniform(-scale, scale, (input_size, hidden_dim)).astype(np.float32)  # Output gate\n",
    "\n",
    "        # LSTM gate biases\n",
    "        self.bf = np.ones((1, hidden_dim), dtype=np.float32)   # Forget gate bias (start with 1s)\n",
    "        self.bi = np.zeros((1, hidden_dim), dtype=np.float32)  # Input gate bias\n",
    "        self.bc = np.zeros((1, hidden_dim), dtype=np.float32)  # Candidate bias\n",
    "        self.bo = np.zeros((1, hidden_dim), dtype=np.float32)  # Output gate bias\n",
    "\n",
    "        # Output layer parameters\n",
    "        self.Wy = np.random.uniform(-scale, scale, (hidden_dim, vocab_size)).astype(np.float32)\n",
    "        self.by = np.zeros((1, vocab_size), dtype=np.float32)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Numerically stable sigmoid function.\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(x, -250, 250)))\n",
    "\n",
    "    def train_step(self, X_batch, Y_batch):\n",
    "        \"\"\"\n",
    "        Perform one training step: forward pass + backward pass + parameter update.\n",
    "        \n",
    "        Args:\n",
    "            X_batch: Input sequences of shape (batch_size, seq_len)\n",
    "            Y_batch: Target sequences of shape (batch_size, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            avg_loss: Average loss across the sequence\n",
    "        \"\"\"\n",
    "        B, T = X_batch.shape\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        h = np.zeros((B, self.hidden_dim), dtype=np.float32)\n",
    "        c = np.zeros((B, self.hidden_dim), dtype=np.float32)\n",
    "\n",
    "        states = []  # Store intermediate states for backpropagation\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Forward pass through each timestep\n",
    "        for t in range(T):\n",
    "            # Get word embeddings for current timestep\n",
    "            x = self.E[X_batch[:, t]]\n",
    "            \n",
    "            # Concatenate input with previous hidden state\n",
    "            z = np.concatenate([x, h], axis=1)\n",
    "\n",
    "            # LSTM gate computations\n",
    "            f = self.sigmoid(np.dot(z, self.Wf) + self.bf)  # Forget gate\n",
    "            i = self.sigmoid(np.dot(z, self.Wi) + self.bi)  # Input gate\n",
    "            c_tilde = np.tanh(np.dot(z, self.Wc) + self.bc)  # Candidate values\n",
    "            o = self.sigmoid(np.dot(z, self.Wo) + self.bo)  # Output gate\n",
    "\n",
    "            # Update cell and hidden states\n",
    "            c = f * c + i * c_tilde  # Cell state\n",
    "            h = o * np.tanh(c)       # Hidden state\n",
    "\n",
    "            # Compute output probabilities\n",
    "            y = np.dot(h, self.Wy) + self.by\n",
    "            \n",
    "            # Numerically stable softmax\n",
    "            y_max = np.max(y, axis=1, keepdims=True)\n",
    "            exp_y = np.exp(y - y_max)\n",
    "            probs = exp_y / np.sum(exp_y, axis=1, keepdims=True)\n",
    "\n",
    "            # Compute cross-entropy loss\n",
    "            correct_probs = probs[np.arange(B), Y_batch[:, t]]\n",
    "            loss = -np.mean(np.log(correct_probs + 1e-8))\n",
    "            total_loss += loss\n",
    "\n",
    "            # Store states for backpropagation\n",
    "            states.append({\n",
    "                'x': x, 'z': z, 'h': h, 'c': c, 'f': f, 'i': i,\n",
    "                'c_tilde': c_tilde, 'o': o, 'probs': probs\n",
    "            })\n",
    "\n",
    "        avg_loss = total_loss / T\n",
    "        \n",
    "        # Backpropagation and parameter update\n",
    "        self._backward_and_update(X_batch, Y_batch, states)\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def _backward_and_update(self, X_batch, Y_batch, states):\n",
    "        \"\"\"\n",
    "        Perform backpropagation through time and update parameters.\n",
    "        \n",
    "        Args:\n",
    "            X_batch: Input sequences\n",
    "            Y_batch: Target sequences  \n",
    "            states: Cached forward pass states\n",
    "        \"\"\"\n",
    "        B, T = X_batch.shape\n",
    "\n",
    "        # Initialize gradient accumulators\n",
    "        dE = np.zeros_like(self.E)\n",
    "        dWf, dWi, dWc, dWo = [np.zeros_like(W) for W in [self.Wf, self.Wi, self.Wc, self.Wo]]\n",
    "        dbf, dbi, dbc, dbo = [np.zeros_like(b) for b in [self.bf, self.bi, self.bc, self.bo]]\n",
    "        dWy, dby = np.zeros_like(self.Wy), np.zeros_like(self.by)\n",
    "\n",
    "        # Initialize gradients flowing from future timesteps\n",
    "        dh_next = np.zeros((B, self.hidden_dim), dtype=np.float32)\n",
    "        dc_next = np.zeros((B, self.hidden_dim), dtype=np.float32)\n",
    "\n",
    "        # Backpropagation through time (reverse order)\n",
    "        for t in reversed(range(T)):\n",
    "            state = states[t]\n",
    "            probs = state['probs']\n",
    "\n",
    "            # Gradient of loss w.r.t. output scores\n",
    "            dy = probs.copy()\n",
    "            dy[np.arange(B), Y_batch[:, t]] -= 1.0\n",
    "            dy /= B\n",
    "\n",
    "            # Gradients for output layer\n",
    "            h = state['h']\n",
    "            dWy += np.dot(h.T, dy)\n",
    "            dby += np.sum(dy, axis=0, keepdims=True)\n",
    "\n",
    "            # Gradient w.r.t. hidden state\n",
    "            dh = np.dot(dy, self.Wy.T) + dh_next\n",
    "\n",
    "            # Extract states for current timestep\n",
    "            c, f, i, c_tilde, o = state['c'], state['f'], state['i'], state['c_tilde'], state['o']\n",
    "            z = state['z']\n",
    "\n",
    "            # Gradients for LSTM gates\n",
    "            do = dh * np.tanh(c)\n",
    "            dc = dh * o * (1 - np.tanh(c)**2) + dc_next\n",
    "\n",
    "            # Handle gradients that depend on previous timestep\n",
    "            if t > 0:\n",
    "                c_prev = states[t-1]['c']\n",
    "                df = dc * c_prev\n",
    "                di = dc * c_tilde\n",
    "                dc_tilde = dc * i\n",
    "                dc_next = dc * f\n",
    "            else:\n",
    "                df = dc * 0  # No previous cell state\n",
    "                di = dc * c_tilde\n",
    "                dc_tilde = dc * i\n",
    "                dc_next = np.zeros_like(dc)\n",
    "\n",
    "            # Apply activation function derivatives\n",
    "            df *= f * (1 - f)        # Sigmoid derivative\n",
    "            di *= i * (1 - i)        # Sigmoid derivative\n",
    "            dc_tilde *= (1 - c_tilde**2)  # Tanh derivative\n",
    "            do *= o * (1 - o)        # Sigmoid derivative\n",
    "\n",
    "            # Accumulate gradients for weight matrices and biases\n",
    "            dWf += np.dot(z.T, df)\n",
    "            dWi += np.dot(z.T, di)\n",
    "            dWc += np.dot(z.T, dc_tilde)\n",
    "            dWo += np.dot(z.T, do)\n",
    "\n",
    "            dbf += np.sum(df, axis=0, keepdims=True)\n",
    "            dbi += np.sum(di, axis=0, keepdims=True)\n",
    "            dbc += np.sum(dc_tilde, axis=0, keepdims=True)\n",
    "            dbo += np.sum(do, axis=0, keepdims=True)\n",
    "\n",
    "            # Gradients w.r.t. input (embeddings and previous hidden state)\n",
    "            dz = (np.dot(df, self.Wf.T) + np.dot(di, self.Wi.T) +\n",
    "                  np.dot(dc_tilde, self.Wc.T) + np.dot(do, self.Wo.T))\n",
    "            dx = dz[:, :self.emb_dim]\n",
    "            dh_next = dz[:, self.emb_dim:]\n",
    "\n",
    "            # Accumulate gradients for word embeddings\n",
    "            for b in range(B):\n",
    "                dE[X_batch[b, t]] += dx[b]\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        grad_norm = np.sqrt(sum(np.sum(g**2) for g in [dE, dWf, dWi, dWc, dWo, dWy]))\n",
    "        if grad_norm > 1.0:\n",
    "            scale = 1.0 / grad_norm\n",
    "            dE *= scale\n",
    "            dWf *= scale; dWi *= scale; dWc *= scale; dWo *= scale\n",
    "            dWy *= scale\n",
    "\n",
    "        # Parameter updates using gradient descent\n",
    "        self.E -= self.lr * dE\n",
    "        self.Wf -= self.lr * dWf\n",
    "        self.Wi -= self.lr * dWi\n",
    "        self.Wc -= self.lr * dWc\n",
    "        self.Wo -= self.lr * dWo\n",
    "        self.bf -= self.lr * dbf\n",
    "        self.bi -= self.lr * dbi\n",
    "        self.bc -= self.lr * dbc\n",
    "        self.bo -= self.lr * dbo\n",
    "        self.Wy -= self.lr * dWy\n",
    "        self.by -= self.lr * dby\n",
    "\n",
    "print(\"LSTM model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation\n",
    "\n",
    "Training loop với adaptive learning rate và theo dõi perplexity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtpNG6Qdn1ij",
    "outputId": "2696c232-1ca9-42e9-ee33-c0cf0cbe373a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with vocab size: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 101/3000 [00:25<05:48,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  100 | loss=6.2809 | ppl=534.3 | best=5.7789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 201/3000 [00:39<05:33,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  200 | loss=5.7844 | ppl=325.2 | best=5.5145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 301/3000 [00:52<05:19,  8.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  300 | loss=5.6929 | ppl=296.8 | best=5.4809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 401/3000 [01:06<05:02,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  400 | loss=5.6111 | ppl=273.4 | best=5.4342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 501/3000 [01:19<04:50,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  500 | loss=5.5581 | ppl=259.3 | best=5.3626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 601/3000 [01:33<04:40,  8.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  600 | loss=5.5143 | ppl=248.2 | best=5.3229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 701/3000 [01:46<04:26,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  700 | loss=5.4784 | ppl=239.5 | best=5.3073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 801/3000 [02:00<04:14,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  800 | loss=5.4544 | ppl=233.8 | best=5.3073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 901/3000 [02:13<04:05,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  900 | loss=5.4315 | ppl=228.5 | best=5.2358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1001/3000 [02:27<03:43,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reducing LR to 0.0035\n",
      "\n",
      "Step 1000 | loss=5.4207 | ppl=226.0 | best=5.2358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1100/3000 [02:41<09:49,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1100 | loss=5.4085 | ppl=223.3 | best=5.2177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1201/3000 [02:55<07:27,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1200 | loss=5.3987 | ppl=221.1 | best=5.2177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1301/3000 [03:09<04:25,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1300 | loss=5.4095 | ppl=223.5 | best=5.2177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1401/3000 [03:22<03:35,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1400 | loss=5.3956 | ppl=220.4 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1501/3000 [03:36<03:04,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1500 | loss=5.3954 | ppl=220.4 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1601/3000 [03:49<02:37,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1600 | loss=5.3922 | ppl=219.7 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1701/3000 [04:03<02:41,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1700 | loss=5.3896 | ppl=219.1 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1801/3000 [04:17<02:40,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1800 | loss=5.3905 | ppl=219.3 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1901/3000 [04:30<02:05,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1900 | loss=5.3835 | ppl=217.8 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2001/3000 [04:44<01:54,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reducing LR to 0.0017\n",
      "\n",
      "Step 2000 | loss=5.3798 | ppl=217.0 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2101/3000 [04:58<01:57,  7.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2100 | loss=5.3842 | ppl=217.9 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 2201/3000 [05:12<01:47,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2200 | loss=5.3850 | ppl=218.1 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 2301/3000 [05:28<01:57,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2300 | loss=5.3913 | ppl=219.5 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2401/3000 [05:42<01:14,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2400 | loss=5.3790 | ppl=216.8 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 2501/3000 [05:56<01:00,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2500 | loss=5.3727 | ppl=215.4 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 2601/3000 [06:10<00:47,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2600 | loss=5.3667 | ppl=214.1 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 2701/3000 [06:23<00:34,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2700 | loss=5.3812 | ppl=217.3 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2801/3000 [06:37<00:23,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2800 | loss=5.3780 | ppl=216.6 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2901/3000 [06:51<00:13,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2900 | loss=5.3671 | ppl=214.2 | best=5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [07:04<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3000 | loss=5.3836 | ppl=217.8 | best=5.1697\n",
      "\n",
      "Final loss: 5.1697\n",
      "Final perplexity: 175.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def perplexity(loss):\n",
    "    \"\"\"\n",
    "    Calculate perplexity from cross-entropy loss.\n",
    "    Perplexity measures how well the model predicts the next word.\n",
    "    \"\"\"\n",
    "    return math.exp(min(loss, 10))  # Clip loss to prevent overflow\n",
    "\n",
    "# Training hyperparameters\n",
    "HIDDEN_DIM = 256    # LSTM hidden dimension\n",
    "BATCH_SIZE = 64     # Sequences per batch\n",
    "LR = 0.005          # Initial learning rate\n",
    "STEPS = 3000        # Total training steps\n",
    "REPORT_EVERY = 100  # Progress reporting frequency\n",
    "\n",
    "print(f\"Training with vocab size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Initialize model and data generator\n",
    "model = SimpleLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    lr=LR\n",
    ")\n",
    "\n",
    "gen = simple_data_generator(token_indices, BATCH_SIZE)\n",
    "\n",
    "# Training tracking variables\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Main training loop\n",
    "for step in trange(1, STEPS + 1):\n",
    "    try:\n",
    "        # Get batch and perform training step\n",
    "        X, Y = next(gen)\n",
    "        loss = model.train_step(X, Y)\n",
    "\n",
    "        # Track loss and update best loss\n",
    "        losses.append(loss)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "\n",
    "        # Adaptive learning rate scheduling\n",
    "        if step == STEPS // 3:  # After 1/3 of training\n",
    "            model.lr *= 0.7\n",
    "            print(f\"\\nReducing LR to {model.lr:.4f}\")\n",
    "        elif step == 2 * STEPS // 3:  # After 2/3 of training\n",
    "            model.lr *= 0.5\n",
    "            print(f\"\\nReducing LR to {model.lr:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at step {step}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Progress reporting\n",
    "    if step % REPORT_EVERY == 0:\n",
    "        # Calculate metrics over recent steps\n",
    "        avg_loss = np.mean(losses[-REPORT_EVERY:])\n",
    "        ppl = perplexity(avg_loss)\n",
    "\n",
    "        print(f\"\\nStep {step:4d} | loss={avg_loss:.4f} | ppl={ppl:.1f} | best={best_loss:.4f}\")\n",
    "\n",
    "# Final results\n",
    "final_ppl = perplexity(best_loss)\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final loss: {best_loss:.4f}\")\n",
    "print(f\"Final perplexity: {final_ppl:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nhận Xét Kết Quả\n",
    "\n",
    "## Tổng Kết\n",
    "\n",
    "Đã hoàn thành implementation LSTM từ scratch cho text generation với:\n",
    "\n",
    "*Kiến trúc:**\n",
    "- Vocabulary: 800 từ  \n",
    "- Hidden dimension: 256\n",
    "- Embedding dimension: 128\n",
    "- Sequence length: 20\n",
    "\n",
    "**Tính năng chính:***\n",
    "- Memory-efficient chunked processing\n",
    "- Gradient clipping để tránh exploding gradients\n",
    "- Adaptive learning rate scheduling\n",
    "- Numerical stability với stable softmax\n",
    "\n",
    "**Kết quả mong đợi:**\n",
    "- Perplexity: 50-150 \n",
    "- Model có thể tạo text có nghĩa cơ bản\n",
    "\n",
    "**Điểm mạnh:**\n",
    "- Implementation từ đầu giúp hiểu rõ thuật toán\n",
    "- Xử lý hiệu quả file dữ liệu lớn"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
