{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==== Load data ====\n",
    "df_train = pd.read_csv(\"train.csv\")  \n",
    "\n",
    "# ==== Preprocess text ====\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "df_train['text_clean'] = df_train['text'].apply(preprocess)\n",
    "\n",
    "# ==== T·∫°o vocab v·ªõi min_count ====\n",
    "sentences = [s.split() for s in df_train['text_clean']]\n",
    "words = [w for s in sentences for w in s]\n",
    "min_count = 2\n",
    "freq = {}\n",
    "for w in words:\n",
    "    freq[w] = freq.get(w, 0) + 1\n",
    "vocab = sorted([w for w in set(words) if freq[w] >= min_count])\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# ==== training pairs Skip-gram ====\n",
    "window_size = 2\n",
    "pairs = []\n",
    "for s in sentences:\n",
    "    s = [w for w in s if w in word2idx]\n",
    "    for i, target in enumerate(s):\n",
    "        target_idx = word2idx[target]\n",
    "        for j in range(max(0, i - window_size), min(len(s), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                context_idx = word2idx[s[j]]\n",
    "                pairs.append((target_idx, context_idx))\n",
    "\n",
    "pairs = np.array(pairs)\n",
    "print(\"Training pairs:\", pairs.shape)\n",
    "\n",
    "# ==== Model params ====\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 50\n",
    "batch_size = 512\n",
    "epochs = 25  \n",
    "num_sampled = 100  \n",
    "\n",
    "# Dataset\n",
    "target_words = tf.constant(pairs[:, 0], dtype=tf.int32)\n",
    "context_words = tf.constant(pairs[:, 1], dtype=tf.int32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((target_words, context_words))\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Variables\n",
    "embeddings = tf.Variable(tf.random.normal([vocab_size, embed_dim], stddev=0.1))\n",
    "nce_weights = tf.Variable(tf.random.normal([vocab_size, embed_dim], stddev=0.1))\n",
    "nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(0.005)  \n",
    "\n",
    "# Checkpoint\n",
    "ckpt = tf.train.Checkpoint(embeddings=embeddings,\n",
    "                           nce_weights=nce_weights,\n",
    "                           nce_biases=nce_biases,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=8)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(f\"‚úÖ Restored from {ckpt_manager.latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"‚ö° Training from scratch\")\n",
    "\n",
    "# ==== Training step ---->\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, x)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(weights=nce_weights,\n",
    "                           biases=nce_biases,\n",
    "                           labels=tf.reshape(y, (-1,1)),\n",
    "                           inputs=embed,\n",
    "                           num_sampled=num_sampled,\n",
    "                           num_classes=vocab_size)\n",
    "        )\n",
    "    grads = tape.gradient(loss, [embeddings, nce_weights, nce_biases])\n",
    "    optimizer.apply_gradients(zip(grads, [embeddings, nce_weights, nce_biases]))\n",
    "    return loss\n",
    "\n",
    "# ==== Training loop ====\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for step, (x, y) in enumerate(tqdm(dataset, desc=f\"Epoch {epoch+1}\")):\n",
    "        loss = train_step(x, y)\n",
    "        total_loss += loss.numpy()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataset):.4f}\")\n",
    "    ckpt_manager.save()\n",
    "    print(\"üíæ Checkpoint saved!\")\n",
    "\n",
    "print(\"‚úÖ Training completed!\")\n",
    "\n",
    "# ==== Save embeddings ---->\n",
    "np.savez_compressed(\"word_embeddings.npz\",\n",
    "                    embeddings=embeddings.numpy(),\n",
    "                    vocab=np.array(vocab))\n",
    "print(\"üíæ Saved embeddings to 'word_embeddings.npz'\")\n",
    "\n",
    "# ==== Test embedding ---->\n",
    "word = \"pizza\"\n",
    "if word in word2idx:\n",
    "    vec = embeddings.numpy()[word2idx[word]]\n",
    "    print(f\"Embedding cho '{word}':\", vec[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "ckpt_path = './checkpoints1/ckpt-26'\n",
    "vocab_path = 'vocab.pkl' \n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['text_clean'] = df_train['text'].apply(preprocess)\n",
    "sentences = [s.split() for s in df_train['text_clean']]\n",
    "words = [w for s in sentences for w in s]\n",
    "min_count = 2\n",
    "freq = {}\n",
    "for w in words:\n",
    "    freq[w] = freq.get(w, 0) + 1\n",
    "vocab = sorted([w for w in set(words) if freq[w] >= min_count])\n",
    "\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "# Load checkpoint\n",
    "embedding_dim = 50  \n",
    "ckpt = tf.train.Checkpoint(embeddings=tf.Variable(tf.random.normal([len(vocab), embedding_dim], stddev=0.1)))\n",
    "ckpt.restore(ckpt_path).expect_partial()\n",
    "np.savez_compressed('word_embeddings.npz', embeddings=ckpt.embeddings.numpy(), vocab=np.array(vocab))\n",
    "print('‚úÖ ƒê√£ xu·∫•t word_embeddings.npz!')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
